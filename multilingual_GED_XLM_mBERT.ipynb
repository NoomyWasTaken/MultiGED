{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#step 1: download everything\n",
        "!pip install transformers==4.28.0\n",
        "!pip install seqeval\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n"
      ],
      "metadata": {
        "id": "wHfUR_e2570K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1.1 import everything\n",
        "import seqeval\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets, DatasetDict\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "yLav8-rX_pTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSC6X6ln5J1M"
      },
      "outputs": [],
      "source": [
        "#step 1.2 is to upload the files\n",
        "#I cloned the entire repo and manually uploaded to Colab\n",
        "#then we open the files up\n",
        "#I don't use the fce german files bc they aren't binary, I only use falko for german"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1.3 use glob to gather all file names\n",
        "#We are using the dev data as \n",
        "train_file_list = glob.glob(\"/content/*train.tsv\")\n",
        "test_file_list = glob.glob(\"/content/*dev.tsv\")\n",
        "print(\"TRAIN FILES:\", train_file_list)\n",
        "print(\"TEST FILES:\", test_file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSdF2LwT5jLx",
        "outputId": "10edd031-3b6e-4e90-cd2a-5f30f4b971ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN FILES: ['/content/cs_geccc_train.tsv', '/content/it_merlin_train.tsv', '/content/de_falko-merlin_train.tsv', '/content/en_fce_train.tsv', '/content/sv_swell_train.tsv']\n",
            "TEST FILES: ['/content/cs_geccc_dev.tsv', '/content/it_merlin_dev.tsv', '/content/en_realec_dev.tsv', '/content/sv_swell_dev.tsv', '/content/en_fce_dev.tsv', '/content/de_falko-merlin_dev.tsv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These dictionaries show the correspondence between the numerical label and the real label\n",
        "label2id = {\"c\": 1, \"i\": 0}\n",
        "id2label = {1: \"c\", 0: \"i\"}"
      ],
      "metadata": {
        "id": "eNtx88eP739G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models make contextual embeddings based on sentence input. Because of this, we need to split the data into sentences, where each token in a sentence has a label attached. This is easy to do because the files we are working with have newlines in between sentences."
      ],
      "metadata": {
        "id": "ygfYXcyi8_yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(fin):\n",
        "  \"\"\"returns a list of sentences given a file\"\"\"\n",
        "\n",
        "  sentences = []\n",
        "  sentence = []\n",
        "  with open(fin, \"r\") as inf:\n",
        "    lang = fin.split(\"_\")[0].split(\"/\")[2]\n",
        "    for line in inf:\n",
        "      if line == \"\\n\":\n",
        "        #if we reach a newline, append the sentence to list and start over\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "      else:\n",
        "        #if we aren't at a newline, split into token and label and add to sent\n",
        "        line = line.strip(\"\\n\")\n",
        "        token, label = line.split(\"\\t\")\n",
        "        token_label = (token, label)\n",
        "        sentence.append(token_label)\n",
        "        \n",
        "  return lang, sentences"
      ],
      "metadata": {
        "id": "WozmnUwh8J55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a multilingual experiment, we are going to combine all the languages together. I'm not actually using this function for this implementation (I just tested on Italian)"
      ],
      "metadata": {
        "id": "e7TISyi6-q9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#not used in the multilingual setting\n",
        "def combine_all_sentences(train_list, test_list):\n",
        "  \"\"\"\n",
        "  reads in all the files and combines them into giant lists of sents\n",
        "  For monolingual setting we will need to do some extra work here to keep track of languages\n",
        "  \"\"\"\n",
        "  train_sents = []\n",
        "  for fin in train_list:\n",
        "    lang, sents = get_sentences(fin)\n",
        "    train_sents += sents\n",
        "  \n",
        "  test_sents = []\n",
        "  for fin in test_list:\n",
        "    lang, sents = get_sentences(fin)\n",
        "    test_sents += sents\n",
        "\n",
        "  return train_sents, test_sents"
      ],
      "metadata": {
        "id": "Bv41BnM0-v2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_dataset(lang, sent_list, test=False):\n",
        "  \"\"\"This function takes a list of lines with each token paired with its label in a tuple\n",
        "  It returns a transformers dataset\n",
        "  \"\"\"\n",
        "  data_dicts = []\n",
        "  #we need an id value for a huggingface dataset\n",
        "  id = 0\n",
        "  for sent in sent_list:\n",
        "    sent_dict = {}\n",
        "    sent_dict['id'] = id\n",
        "    sent_dict['lang'] = lang\n",
        "    id += 1\n",
        "\n",
        "    tokens = [s[0] for s in sent]\n",
        "    labels = [s[1] for s in sent]\n",
        "\n",
        "    #convert \"c\" and \"i\" labels to numerical ones\n",
        "    nums = [label2id[t] for t in labels]\n",
        "    sent_dict[\"tokens\"] = tokens\n",
        "    sent_dict[\"labels\"] = nums\n",
        "    #append sentence dict to list\n",
        "    data_dicts.append(sent_dict)\n",
        "\n",
        "  #convert list of dictionaries into Dataset\n",
        "  combined_dataset = Dataset.from_list(data_dicts)\n",
        "\n",
        "\n",
        "  if not test:\n",
        "    dataset = combined_dataset.train_test_split(test_size=0.2, shuffle=True, seed=34)\n",
        "    dataset[\"dev\"] = dataset[\"test\"]\n",
        "    del dataset[\"test\"]\n",
        "  \n",
        "  else:\n",
        "    dataset = combined_dataset\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "FiGAf63D5vt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_datasets_multiling(train_list, test_list):\n",
        "  \"\"\"we read in each file as a dataset individually (with the language recorded)\n",
        "  then concatenate all the datasets. The test set in the dataset is the dev set\n",
        "  from the shared task (since the ST test set doesn't have gold labels released\n",
        "  The dev set for the dataset is a random .2 split from the training ST set.\"\"\"\n",
        "\n",
        "  train_ds_list = []\n",
        "  test_ds_list = []\n",
        "\n",
        "  for fin in train_list:\n",
        "    lang, sents = get_sentences(fin)\n",
        "    train_ds = list_to_dataset(lang, sents)\n",
        "    train_ds_list.append(train_ds)\n",
        "\n",
        "  for fin2 in test_list:\n",
        "    lang, sents = get_sentences(fin2)\n",
        "    print(lang)\n",
        "    test_ds = list_to_dataset(lang, sents, test=True)\n",
        "    test_ds_list.append(test_ds)\n",
        "  \n",
        "  \n",
        "  trains = concatenate_datasets([d[\"train\"] for d in train_ds_list]).shuffle(seed=420)\n",
        "\n",
        "  devs = concatenate_datasets([d[\"dev\"] for d in train_ds_list]).shuffle(seed=420)\n",
        "\n",
        "  tests = concatenate_datasets([d for d in test_ds_list]).shuffle(seed=420)\n",
        "  print(tests[0][\"lang\"], tests[2000][\"lang\"])\n",
        "  print(trains, devs, tests)\n",
        "\n",
        "  full_train_ds = {\n",
        "      \"train\": trains,\n",
        "      \"dev\": devs,\n",
        "      \"test\": tests\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  return DatasetDict(full_train_ds)"
      ],
      "metadata": {
        "id": "XisP_g0M-pwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "B989k8MRCkep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_files_from_dataset(dev_set):\n",
        "  \"\"\"\n",
        "  create ref and hyp files for eval script\n",
        "  this will write files to the colab directory, you can download them and then run eval.py\n",
        "  this takes the language and writes each language to a diff file\n",
        "  \"\"\"\n",
        "  predictions = trainer.predict(dev_set)\n",
        "  true_predictions, true_labels = return_predictions(predictions)\n",
        "  with open(\"/content/cs_output_hyp4t.tsv\", \"w\", encoding=\"utf-8\") as cs_hyp:\n",
        "    with open(\"/content/en_output_hyp4t.tsv\", \"w\", encoding=\"utf-8\") as en_hyp:\n",
        "      with open(\"/content/de_output_hyp4t.tsv\", \"w\", encoding=\"utf-8\") as de_hyp:\n",
        "        with open(\"/content/it_output_hyp4t.tsv\", \"w\", encoding=\"utf-8\") as it_hyp:\n",
        "          with open(\"/content/sv_output_hyp4t.tsv\", \"w\", encoding=\"utf-8\") as sv_hyp:\n",
        "            with open(\"/content/cs_output_ref4t.tsv\", \"w\", encoding=\"utf-8\") as cs_ref:\n",
        "              with open(\"/content/en_output_ref4t.tsv\", \"w\", encoding=\"utf-8\") as en_ref:\n",
        "                with open(\"/content/de_output_ref4t.tsv\", \"w\", encoding=\"utf-8\") as de_ref:\n",
        "                  with open(\"/content/it_output_ref4t.tsv\", \"w\", encoding=\"utf-8\") as it_ref:\n",
        "                    with open(\"/content/sv_output_ref4t.tsv\", \"w\", encoding=\"utf-8\") as sv_ref:\n",
        "\n",
        "                      #hyp files\n",
        "                      hypes = {\"cs\": cs_hyp,\n",
        "                        \"en\": en_hyp,\n",
        "                        \"de\": de_hyp,\n",
        "                        \"it\": it_hyp,\n",
        "                        \"sv\": sv_hyp}\n",
        "\n",
        "                      #ref files\n",
        "                      refs = {\"cs\": cs_ref,\n",
        "                      \"en\": en_ref,\n",
        "                      \"de\": de_ref,\n",
        "                      \"it\": it_ref,\n",
        "                      \"sv\": sv_ref}\n",
        "\n",
        "                      for i in range(len(dev_set)):\n",
        "                        lang = dev_set[i][\"lang\"]\n",
        "                        hyp_file = hypes[lang]\n",
        "                        ref_file = refs[lang]\n",
        "                        tokens = dev_set[i][\"tokens\"]\n",
        "                        labs = true_labels[i]\n",
        "                        preds = true_predictions[i]\n",
        "                        # print(lang)\n",
        "                        for j in range(len(tokens)):\n",
        "                          tok = tokens[j]\n",
        "                          p = preds[j]\n",
        "                          l = labs[j]\n",
        "                          ref_file.write(tok + \"\\t\" + l + \"\\n\")\n",
        "                          hyp_file.write(tok + \"\\t\" + p + \"\\n\")\n",
        "                        ref_file.write(\"\\n\")\n",
        "                        hyp_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "msaCu2ZN5gSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    #not working exactly as intended, but we're using the eval.py script for evaluation anyway\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "pn-pA3pZGJzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_predictions(p):\n",
        "    \"\"\"\n",
        "    returns the predicted and true labels from the test set. \n",
        "    Used for writing the hyp and ref files for testing\n",
        "    \"\"\"\n",
        "    predictions, labels, metrics = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    \n",
        "    return true_predictions, true_labels"
      ],
      "metadata": {
        "id": "IEBoe36LvSy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the functions are done, here is the actual experiment"
      ],
      "metadata": {
        "id": "xFVUhg_R5ix9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = [\"i\",\"c\"]"
      ],
      "metadata": {
        "id": "pT9TE75vF4_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\") #change to \"bert-base-multilingual-cased\" if you want to try mBERT"
      ],
      "metadata": {
        "id": "fSw2wuI4LdMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer) #used for dynamic padding of batches"
      ],
      "metadata": {
        "id": "X9OuJ1dkVds8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "XWWukqSACSCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we actually load in the files into a huggingface dataset, and then tokenize the dataset\n",
        "train_ds = combine_datasets_multiling(train_file_list, test_file_list)\n",
        "tokenized_dataset = train_ds.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "Oh_fLkRbD3tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset[\"test\"][0]['lang'] #just for taking a look"
      ],
      "metadata": {
        "id": "SWzuaUkSCJtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ") #change to \"bert-base-multilingual-cased\" if you want to try mBERT"
      ],
      "metadata": {
        "id": "NsNrXG6YDHlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec28677-9e52-49f0-b22d-1109cf00980e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training args for the trainer, modify hyperparams if desired\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_model\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.02,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False\n",
        ") "
      ],
      "metadata": {
        "id": "Xo3fKnsFDMBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiating the trainer with tokenizer, data collator, and train/dev datasets from above\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"dev\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "oXKQX2ItDTJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#actually runs the training. Will take several hours depending on GPU usage\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1wfHTC-1DaVi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "993a95e8-7f87-4b0b-f1ca-952fadcf73a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='90520' max='90520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [90520/90520 1:50:16, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.239100</td>\n",
              "      <td>0.248782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.911220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.217200</td>\n",
              "      <td>0.228125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.918907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.186600</td>\n",
              "      <td>0.230955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.920423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.160200</td>\n",
              "      <td>0.246775</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.921152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.269524</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.921777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.126100</td>\n",
              "      <td>0.277462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.921916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.112600</td>\n",
              "      <td>0.299732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.101300</td>\n",
              "      <td>0.302014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.922118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.320503</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.922207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.088400</td>\n",
              "      <td>0.336628</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.922153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: c seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: c seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: c seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: c seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: c seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=90520, training_loss=0.1493178774548799, metrics={'train_runtime': 6616.3213, 'train_samples_per_second': 109.441, 'train_steps_per_second': 13.681, 'total_flos': 1.5678243280365624e+16, 'train_loss': 0.1493178774548799, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_files_from_dataset(tokenized_dataset[\"test\"]) #creates the hyp and ref files for the test_dataset"
      ],
      "metadata": {
        "id": "i7GAztgZ1vas"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}